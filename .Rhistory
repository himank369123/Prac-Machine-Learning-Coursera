source('~/R/COURSERA DS/8.machine learning/CrossValidation.R')
source('~/R/COURSERA DS/8.machine learning/CrossValidation.R')
##POISOON REGRSSION:
#### THe outcome follow poission dist with mean u
#### link function is log(u)
#### log(u)=b0+b1x
#### e(b0+b1x)=u where u is mean or geometric mean for outocomes.
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
##fitting linear model:
lmfit<-lm(data=gaData,visits~julian)
summary(lmfit)
#suggests 0.028% increase in visits per day.
g<-g+geom_smooth(method="lm",color='red',se=F,size=2)
g
## now fitting poisson regression model:
glmfit<-glm(data=gaData,visits~julian,family="poisson")
summary(glmfit)
exp(coef(glmfit))
## 0.002% incrase in visits per day.
g<-g+geom_smooth(method="glm",method.args=list(family="poisson"),se=F,size=2)
g
## we can see a sligt curve in case of glm
##we can model proportions too.
##for example proportion of webhits originating from simply statistics compared to toal visits.
## then we want to model log(simplystats/visits)=b0+b1x
##it turns out to be normal model +offset . where offset is log of denominator or total count.
glmfit2<-glm(data=gaData,simplystats~julian,offset=log(visits+1),family="poisson")
summary(glmfit2)
exp(coef(glmfit2))
plot(gaData$julian,glmfit2$fitted,col="blue",pch=19)
points(gaData$julian,glmfit$fitted,col="red",pch=19)
##glmfit vs glmfit2
##actual points of proportion vs model(line)
plot(gaData$julian,gaData$simplystats/(gaData$visits+1),col="grey")
lines(gaData$julian,glmfit2$fitted/(gaData$visits+1),col="blue")
load("C:/Users/himank/Documents/R/COURSERA DS/7.regression models/gaData.rda")
##POISOON REGRSSION:
#### THe outcome follow poission dist with mean u
#### link function is log(u)
#### log(u)=b0+b1x
#### e(b0+b1x)=u where u is mean or geometric mean for outocomes.
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
##fitting linear model:
lmfit<-lm(data=gaData,visits~julian)
summary(lmfit)
#suggests 0.028% increase in visits per day.
g<-g+geom_smooth(method="lm",color='red',se=F,size=2)
g
## now fitting poisson regression model:
glmfit<-glm(data=gaData,visits~julian,family="poisson")
summary(glmfit)
exp(coef(glmfit))
## 0.002% incrase in visits per day.
g<-g+geom_smooth(method="glm",method.args=list(family="poisson"),se=F,size=2)
g
## we can see a sligt curve in case of glm
##we can model proportions too.
##for example proportion of webhits originating from simply statistics compared to toal visits.
## then we want to model log(simplystats/visits)=b0+b1x
##it turns out to be normal model +offset . where offset is log of denominator or total count.
glmfit2<-glm(data=gaData,simplystats~julian,offset=log(visits+1),family="poisson")
summary(glmfit2)
exp(coef(glmfit2))
plot(gaData$julian,glmfit2$fitted,col="blue",pch=19)
points(gaData$julian,glmfit$fitted,col="red",pch=19)
##glmfit vs glmfit2
##actual points of proportion vs model(line)
plot(gaData$julian,gaData$simplystats/(gaData$visits+1),col="grey")
lines(gaData$julian,glmfit2$fitted/(gaData$visits+1),col="blue")
##POISOON REGRSSION:
#### THe outcome follow poission dist with mean u
#### link function is log(u)
#### log(u)=b0+b1x
#### e(b0+b1x)=u where u is mean or geometric mean for outocomes.
library(ggplot2)
gaData$julian<-julian(gaData$date)
head(gaData)
g<-ggplot(data=gaData,aes(x=julian,y=visits))
g<-g+geom_point()
g
## now fitting poisson regression model:
glmfit<-glm(data=gaData,visits~julian,family="poisson")
summary(glmfit)
exp(coef(glmfit))
## 0.002% incrase in visits per day.
g<-g+geom_smooth(method="glm",method.args=list(family="poisson"),se=F,size=2)
g
data(iris)
library(ggplot2)
table(iris$Species)
partition<-createDataPartition(y=iris$Species,p=0.75,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
dim(training)
qplot(data=training,Petal.Width,Petal.Length,col=Species)
qplot(data=training,Petal.Width,Sepal.Width,col=Species)
library(caret)
#modelling with classification tree
model<-train(Species~.,method='rpart',data=training)
model$finalModel
#plotting classification tree dendogram
plot(model$finalModel,uniform=T)
text(model$finalModel,use.n=T,all=T,cex=.8)
#prettier dendogram with rattle package fancyRpartplot function
install.packages("rattle")
library(rattle)
fancyRpartPlot(model$finalModel)
#prediction
predict(model,newdata=testing)
confusionMatrix(predict(model,newdata=testing),testing$Species)
#note : classificatoin trees work well with non linear settings.exceptionally for categorizing problems where outcome is a facotr of multiple levls.
#they might overfit training model due to interaction in variables.
#transformations are not much important.
#easy to interpret & visualize and implicit feature selection
#non linearity isnt a problem and transformations are not needes
data(mtcars)
model1<-train(data=mtcars,mpg~.,method='rpart')
model1
fancyRpartPlot(model1$finalModel)
mean(mtcars$mpg)
qplot(data=mtcars,cyl,mpg)
mean(mtcars$mpg[])
mean(mtcars[mtcars$cyl>=5,]$mpg)
library(ggplot2)
table(iris$Species)
partition<-createDataPartition(y=iris$Species,p=0.75,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
dim(training)
qplot(data=training,Petal.Width,Petal.Length,col=Species)
qplot(data=training,Petal.Width,Sepal.Width,col=Species)
qplot(data=training,Petal.Width,Petal.Length,col=Species)
qplot(data=training,Petal.Width,Sepal.Width,col=Species)
library(caret)
#modelling with classification tree
model<-train(Species~.,method='rpart',data=training)
model$finalModel
#plotting classification tree dendogram
plot(model$finalModel,uniform=T)
text(model$finalModel,use.n=T,all=T,cex=.8)
fancyRpartPlot(model$finalModel)
#prediction
predict(model,newdata=testing)
confusionMatrix(predict(model,newdata=testing),testing$Species)
#prediction
predict(model,newdata=testing)
confusionMatrix(predict(model,newdata=testing),testing$Species)
#Bagging or BootStrap aggregating.
##useful for non linear functions.
##resample cases and recalculate predictions and then avg them.
##it reduces variance.
library(ElemStatLearn)
#Bagging or BootStrap aggregating.
##useful for non linear functions.
##resample cases and recalculate predictions and then avg them.
##it reduces variance.
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("ozone")
ozone<-ozone[order(ozone$ozone),]
head(ozone)
ll<-matrix(NA,nrow=10,ncol=155)
ss<-sample(1:nrow(ozone),replace=T)
ll<-matrix(NA,nrow=10,ncol=155)
for(i in 1:10){
ss<-sample(1:nrow(ozone),replace=T)
ozone0<-ozone[ss,];
ozone0<-ozone0[order(ozone$ozone),]
loess0<-loess(temperature~ozone,data=ozone0,span=0.2)
ll[i,]<-predict(loess0,newdata=data.frame(ozone=1:155))
}
plot(ozone$ozone,ozone$temperature,pch=19,cex=0.5)
for(i in 1:10){lines(1:155,ll[i,],col='grey',lwd=2)}
for(i in 1:10){lines(ozone$ozone,ll[i,],col='grey',lwd=2)}
for(i in 1:10){lines(1:155,ll[i,],col='grey',lwd=2)}
for(i in 1:10){lines(1:155,ll[i,],col='grey',lwd=2)}
for(i in 1:10){lines(1:155,ll[i,],col='grey',lwd=2)}
ll[i]
ll[1,]
lines(1:155,apply(ll,2,mean),col='red',lwd=2)
#bagging can be done in train function with method parameter.
library(caret)
data(iris)
library(ggplot2)
table(iris$Species)
partition<-createDataPartition(y=iris$Species,p=0.75,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
dim(training)
qplot(data=training,Petal.Width,Petal.Length,col=Species)
qplot(data=training,Petal.Width,Sepal.Width,col=Species)
model<-train(Species~.,method='treebag',data=training)
model$finalModel
model$finalModel
library(rattle)
fancyRpartPlot(model$finalModel)
text(model$finalModel,use.n=T,all=T,cex=.8)
plot(model$finalModel,uniform=T)
text(model$finalModel,use.n=T,all=T,cex=.8)
plot(model$finalModel,uniform=T)
model$finalModel
summary(model$finalModel)
predict(model$finalModel,newdata=training)
confusionMatrix(predict(model$finalModel,newdata=testing),testing$Species)
#model without bagging
model<-train(Species~.method='rpart',data=training)
#model without bagging
model<-train(Species~.,method='rpart',data=training)
confusionMatrix(predict(model$finalModel,newdata=testing),testing$Species)
confusionMatrix(predict(model$finalModel,newdata=testing),testing$Species)
#model without bagging
model<-train(Species~.,method='rpart',data=training)
confusionMatrix(predict(model$finalModel,newdata=testing),testing$Species)
predict(model$finalModel,newdata=testing)
predict(model,newdata=testing)
confusionMatrix(predict(model,newdata=testing),testing$Species)
#model without bagging
model<-train(Species~.,method='rpart',data=training)
predict(model,newdata=testing)
confusionMatrix(predict(model,newdata=testing),testing$Species)
#model with bagging
model<-train(Species~.,method='treebag',data=training)
confusionMatrix(predict(model$finalModel,newdata=testing),testing$Species)
confusionMatrix(predict(model,newdata=testing),testing$Species)
#model with bagging
model<-train(Species~.,method='treebag',data=training)
confusionMatrix(predict(model,newdata=testing),testing$Species)
plot(model$finalModel$mtrees)
plot(model$finalModel$mtrees)[1]
plot(model$finalModel$mtrees[1])
model$finalModel$mtrees[1]
model$finalModel$mtrees[1]$btree
model$finalModel$mtrees[1]$
model$finalModel$mtrees[1]
model$finalModel$mtrees[1]
model$finalModel$mtrees[1][[1]]
model$finalModel$mtrees[1][[1]]$btree
plot(model$finalModel$mtrees[1][[1]]$btree)
text(model$finalModel$mtrees[1][[1]]$btree,all=T)
text(model$finalModel$mtrees[1][[1]]$btree,all=T,use.n=T)
fancyRpartPlot(model$finalModel$mtrees[1][[1]]$btree)
fancyRpartPlot(model$finalModel$mtrees[[1]]$btree)
plot(model$finalModel$mtrees[1][[1]]$btree)
text(model$finalModel$mtrees[1][[1]]$btree,all=T,use.n=T)
fancyRpartPlot(model$finalModel$mtrees[[1]]$btree)
library(caret)
library(ggplot2)
training<-createDataPartition(y=iris$Species,p=0.7,list=F)
partition<-createDataPartition(y=iris$Species,p=0.7,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
model<-train(Species~.,data=training,method='rf',prox=T)
model
getTree(model$finalModel,k=2)
install.packages(randomForest)
library(randomForest)
getTree(model$finalModel,k=2)
model
pred<-predict(model,testing)
confusionMatrix(testing$Species,pred)
model<-train(Species~.,data=training,method='rf',prox=T)
confusionMatrix(testing$Species,pred)
##plotting missed values
predright<-pred==testing$Species
qplot(Petal.Width,Petal.Length,colour=predright,data=testing)
trainData<-read.csv("C:/Users/himank/Documents/R/titanic/train.csv")
testData<-read.csv("C:/Users/himank/Documents/R/titanic/test.csv")
trainData$Pclass<-factor(trainData$Pclass)
trainData$Sex<-factor(trainData$Sex)
trainData$Survived<-factor(trainData$Survived)
testData$Pclass<-factor(testData$Pclass)
testData$Sex<-factor(testData$Sex)
library(caret)
names(trainData)
preobj<-preProcess(trainData[,-2],method="knnImpute")
train<-predict(preobj,trainData[,-2])
test<-predict(preobj,testData)
names(test)
#bagging
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='treebag')
#bagging
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='treebag')
train<-predict(preobj,trainData[,-2])
test<-predict(preobj,testData)
names(test)
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='rpart')
model
pred<-predict(model,test)
predictions<-data.frame(testData$PassengerId,pred)
names(predictions)<-c("PassengerId","Survived")
predictions
write.csv(predictions,file="C:/Users/himank/Documents/R/titanic/predictionsDecisionTree.csv",row.names = F,quote = F)
library(caret)
library(ggplot2)
partition<-createDataPartition(y=iris$Species,p=0.7,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
model<-train(Species~.,data=training,method='rf',prox=T)
model
#bagging
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='treebag')
model
#bagging
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='treebag')
names(train)
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='rpart')
model
library(caret)
names(trainData)
preobj<-preProcess(trainData[,-2],method="knnImpute")
train<-predict(preobj,trainData[,-2])
test<-predict(preobj,testData)
names(test)
names(train)
trainData<-read.csv("C:/Users/himank/Documents/R/titanic/train.csv")
testData<-read.csv("C:/Users/himank/Documents/R/titanic/test.csv")
trainData$Pclass<-factor(trainData$Pclass)
trainData$Sex<-factor(trainData$Sex)
testData$Pclass<-factor(testData$Pclass)
trainData$Survived<-factor(trainData$Survived)
testData$Sex<-factor(testData$Sex)
library(caret)
names(trainData)
preobj<-preProcess(trainData[,-2],method="knnImpute")
train<-predict(preobj,trainData[,-2])
test<-predict(preobj,testData)
names(train)
train<-predict(preobj,trainData)
trainData<-read.csv("C:/Users/himank/Documents/R/titanic/train.csv")
testData<-read.csv("C:/Users/himank/Documents/R/titanic/test.csv")
trainData$Pclass<-factor(trainData$Pclass)
trainData$Survived<-factor(trainData$Survived)
testData$Pclass<-factor(testData$Pclass)
testData$Sex<-factor(testData$Sex)
names(trainData)
trainData$Sex<-factor(trainData$Sex)
preobj<-preProcess(trainData[,-2],method="knnImpute")
train<-predict(preobj,trainData)
test<-predict(preobj,testData)
library(caret)
names(train)
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='rpart')
model
pred<-predict(model,test)
predictions<-data.frame(testData$PassengerId,pred)
names(predictions)<-c("PassengerId","Survived")
predictions
write.csv(predictions,file="C:/Users/himank/Documents/R/titanic/predictionsDecisionTree.csv",row.names = F,quote = F)
#bagging
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='treebag')
model
pred<-predict(model,test)
predictions<-data.frame(testData$PassengerId,pred)
names(predictions)<-c("PassengerId","Survived")
predictions
write.csv(predictions,file="C:/Users/himank/Documents/R/titanic/predictionsBaggedTree.csv",row.names = F,quote = F)
#random forest
model<-train(data=train,Survived~Pclass+Sex+Age+SibSp+Parch+Fare+Embarked,method='rf',prox=T)
model
pred<-predict(model,test)
predictions<-data.frame(testData$PassengerId,pred)
names(predictions)<-c("PassengerId","Survived")
predictions
write.csv(predictions,file="C:/Users/himank/Documents/R/titanic/predictionsrandomForest.csv",row.names = F,quote = F)
##boosting
##use weak classifiers and weigh them and add them up to get strong classifiers.
library(ISLR)
data(Wage)
library(ggplot2)
library(caret)
Wage<-subset(Wage,select=-c(logwage))
partition<-createDataPartition(y=Wage$wage,p=0.7,list=F)
training<-Wage[partition,]
testing<-Wage[-partition,]
#gbm uses boosting with trees
#can use gbm/mboost/ada/gamBoost
model<-train(wage~.,method="gbm",data=training,verbose=F)
model
qplot(predict(model,testing),wage,data=testing)
#model based prediction
#Linear discriminant analysis, Quadratic discriminant analysis,Naive Bayes
data(iris)
library(ggplot2)
partition<-createDataPartition(y=iris$Species,p=0.7,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
modelLDA<-train(Species~.,data=training,method='lda')
modelNB<-train(Species~.,data=training,method='nb')
modelNB<-train(Species~.,data=training,method='nb')
predict(modelLDA,testing)
confusionMatrix(testing$Species,predict(modelLDA,testing))
confusionMatrix(testing$Species,predict(modelnb,testing)
confusionMatrix(testing$Species,predict(modelNB,testing)
confusionMatrix(testing$Species,predict(modelNB,testing)
confusionMatrix(testing$Species,predict(modelNB,testing))
confusionMatrix(testing$Species,predict(modelLDA,testing))
confusionMatrix(testing$Species,predict(modelNB,testing))
confusionMatrix(testing$Species,predict(modelLDA,testing))
confusionMatrix(testing$Species,predict(modelNB,testing))
confusionMatrix(testing$Species,predict(modelLDA,testing))
predict(modelLDA,testing)==testing$Species
equality<-predict(modelLDA,testing)==testing$Species
qplot(Petal.Width,Sepal.Width,colour=equality,data=testing)
equality<-predict(modelLDA,testing)==testing$Species
equality
qplot(Petal.Width,Sepal.Width,colour=equality,data=testing)
qplot(Petal.Width,Sepal.Width,col=equality,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality,data=testing)
equality2<-predict(modelNB,testing)==testing$Species
qplot(Petal.Width,Sepal.Width,color=equality2,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality1,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality2,data=testing)
equality1<-predict(modelLDA,testing)==testing$Species
equality2<-predict(modelNB,testing)==testing$Species
qplot(Petal.Width,Sepal.Width,color=equality1,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality2,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality1,data=testing)
qplot(Petal.Width,Sepal.Width,color=equality2,data=testing)
qplot(Petal.Width,Petal.Length,color=equality2,data=testing)
qplot(Petal.Width,Petal.Length,color=equality1,data=testing)
qplot(Petal.Width,Petal.Length,color=equality2,data=testing)
#refer: https://www.coursera.org/learn/practical-machine-learning/lecture/XKsl6/random-forests
#create multiple trees with bootstraped resamples and at every split resample the variables.
#highly accurate for competetions.
library(caret)
library(ggplot2)
partition<-createDataPartition(y=iris$Species,p=0.7,list=F)
training<-iris[partition,]
testing<-iris[-partition,]
model<-train(Species~.,data=training,method='rf',prox=T)
model
library(randomForest)
#get second tree of model.
getTree(model$finalModel,k=2)
pred<-predict(model,testing)
confusionMatrix(testing$Species,pred)
##plotting missed values
predright<-pred==testing$Species
qplot(Petal.Width,Petal.Length,colour=predright,data=testing)
irisP<-classCenter(training[,c(3,4)],training$Species,model$finalModel$prox)
irisP<-classCenter(training[,c(3,4)],training$Species,model$finalModel$prox)
irisP
irisP<-as.data.frame(irisP)
irisP
irisP<-as.data.frame(irisP);irisP$Species<-rownames(irisP)
irisP
rownames(irisP)
irisP<-as.data.frame(irisP);irisP$Species<-unique(training$Species)
irisP
p<-qplot(Petal.Width,Petal.Length,col=Species,data=training)
p+geom_point(aes(x=Petal.Width,y=Petal.Length,col=Species),size=5,shape=4,data=irisP)
?classCenter
irisP<-classCenter(training,training$Species,model$finalModel$prox)
irisP
irisP<-classCenter(training,training$Species,model$finalModel$prox)
irisP
[,c(3,4)
irisP<-classCenter(training[,c(3,4)],training$Species,model$finalModel$prox)
irisP
